<!-- Generated by SpecStory -->

# Proceed with subtask 7.3 (2025-05-30 12:50:41)

_**User**_

Please proceed with subtask 7.3

---

_**Assistant**_



---

Okay, let's get started on subtask 7.3.

First, I need to understand the requirements of this subtask. I'll retrieve its details.

---



---

Subtask 7.3 is "Confidence Scoring System Design, Implementation, and Testing". This involves designing scoring metrics, implementing the scoring algorithm, and testing it.

I'll now mark subtask 7.3 as `in-progress`.

---



---

Now, let's begin the iterative subtask implementation process.

**Iteration 1: Initial Exploration & Planning**

The goal is to design, implement, and test a confidence scoring system.

1.  **Design Scoring Metrics**:
    *   Data Completeness: How much of the expected data is present? (e.g., all fields in an API spec).
    *   Source Reliability: Is the source known for accuracy? (e.g., official documentation vs. a blog post).
    *   Consistency: Do multiple sources agree?
    *   Recency: How up-to-date is the information?
    *   Validation: Did the data pass any automated checks (e.g., linting for OpenAPI specs)?

2.  **Implementation**:
    *   This will likely be a new Python module, perhaps `api_discovery_tool/scoring.py`.
    *   It will need a function, say `calculate_confidence(result_data)`, that takes processed discovery results and returns a score (e.g., 0-100 or a qualitative label like "High", "Medium", "Low").
    *   The `result_data` might be a dictionary or an object containing the API information and metadata about its discovery.

3.  **Testing**:
    *   Create unit tests in `api_discovery_tool/processing/tests/test_scoring.py`.
    *   Test cases should cover:
        *   High confidence scenarios (e.g., complete data from a reliable source).
        *   Low confidence scenarios (e.g., incomplete data, unknown source).
        *   Edge cases and varied inputs.

Let's log this initial plan.

---



---

I'll verify the plan has been logged.

---



---

The plan is logged.

Now, I will create the `api_discovery_tool/scoring.py` file with a basic structure and the `calculate_confidence` function.

---

```diff
- 
+ import datetime
+ 
+ # Define constants for source reliability scores
+ SOURCE_RELIABILITY_SCORES = {
+     'official_doc': 0.9,
+     'known_api_db': 0.8,
+     'partner_api': 0.75, # Assuming partner APIs are generally reliable
+     'internal_source': 0.95, # Internal sources might be highly reliable
+     'blog': 0.5,
+     'forum': 0.3,
+     'code_repository': 0.6, # e.g., GitHub search results
+     'web_search': 0.4, # General web search results
+     'unknown': 0.2,
+ }
+ 
+ # Define constants for weighting factors
+ WEIGHTS = {
+     'completeness': 0.3,
+     'reliability': 0.4,
+     'recency': 0.15,
+     'validation': 0.15,
+ }
+ 
+ # Define recency score parameters
+ MAX_RECENCY_SCORE = 1.0
+ RECENCY_DECAY_RATE_PER_YEAR = 0.1 # Score decays by 10% per year of age
+ 
+ def calculate_completeness_score(fields_present: int, total_expected_fields: int) -> float:
+     \"\"\"Calculates a score based on data completeness.\"\"\"
+     if total_expected_fields <= 0:
+         return 0.0  # Avoid division by zero, or handle as low confidence
+     completeness_ratio = fields_present / total_expected_fields
+     return min(max(completeness_ratio, 0.0), 1.0) # Ensure score is between 0 and 1
+ 
+ def calculate_reliability_score(source_type: str) -> float:
+     \"\"\"Calculates a score based on source reliability.\"\"\"
+     return SOURCE_RELIABILITY_SCORES.get(source_type.lower(), SOURCE_RELIABILITY_SCORES['unknown'])
+ 
+ def calculate_recency_score(discovery_timestamp: datetime.datetime | None) -> float:
+     \"\"\"Calculates a score based on the recency of the information.\"\"\"
+     if discovery_timestamp is None:
+         return 0.5 # Neutral score if timestamp is unknown
+ 
+     age_in_days = (datetime.datetime.now(datetime.timezone.utc) - discovery_timestamp).days
+     if age_in_days < 0: # Timestamp is in the future, treat as very recent or an error
+         return MAX_RECENCY_SCORE
+ 
+     age_in_years = age_in_days / 365.25
+     decay_factor = RECENCY_DECAY_RATE_PER_YEAR * age_in_years
+     recency_score = MAX_RECENCY_SCORE - decay_factor
+     return min(max(recency_score, 0.0), MAX_RECENCY_SCORE) # Ensure score is between 0 and MAX_RECENCY_SCORE
+ 
+ def calculate_validation_score(validated: bool | None) -> float:
+     \"\"\"Calculates a score based on whether the data was validated.\"\"\"
+     if validated is None:
+         return 0.5 # Neutral score if validation status is unknown
+     return 1.0 if validated else 0.0
+ 
+ def calculate_confidence(result_data: dict) -> float:
+     \"\"\"
+     Calculates a confidence score for a given discovery result.
+ 
+     Args:
+         result_data (dict): A dictionary containing processed result information.
+             Expected keys in result_data['metadata']:
+             - 'source_type': (str) Type of the source (e.g., 'official_doc').
+             - 'fields_present': (int) Number of fields found.
+             - 'total_expected_fields': (int) Total number of expected fields.
+             - 'discovery_timestamp': (datetime.datetime, optional) When the info was found.
+             - 'validated': (bool, optional) If the data passed automated checks.
+ 
+     Returns:
+         float: A confidence score between 0.0 and 1.0.
+     \"\"\"
+     metadata = result_data.get('metadata', {})
+ 
+     completeness = calculate_completeness_score(
+         metadata.get('fields_present', 0),
+         metadata.get('total_expected_fields', 0)
+     )
+ 
+     reliability = calculate_reliability_score(
+         metadata.get('source_type', 'unknown')
+     )
+ 
+     recency = calculate_recency_score(
+         metadata.get('discovery_timestamp')
+     )
+ 
+     validation = calculate_validation_score(
+         metadata.get('validated')
+     )
+ 
+     # Weighted sum of scores
+     # Ensure weights sum to 1.0 if modifying them. For now, we assume they do.
+     # Normalization: Each individual score is already between 0 and 1.
+     # The weighted sum will also be between 0 and 1 if weights sum to 1.
+     # Current weights sum: 0.3 + 0.4 + 0.15 + 0.15 = 1.0
+ 
+     confidence_score = (
+         completeness * WEIGHTS['completeness'] +
+         reliability * WEIGHTS['reliability'] +
+         recency * WEIGHTS['recency'] +
+         validation * WEIGHTS['validation']
+     )
+     
+     # Consistency bonus is not yet implemented as it requires cross-result analysis.
+     # For now, the score is capped at 1.0.
+     return min(max(confidence_score, 0.0), 1.0)
+ 
+ if __name__ == '__main__':
+     # Example Usage (for direct testing of this module)
+     print("Running example confidence score calculations...")
+ 
+     example_result_high_confidence = {
+         'metadata': {
+             'source_type': 'official_doc',
+             'fields_present': 10,
+             'total_expected_fields': 10,
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=30),
+             'validated': True
+         }
+     }
+     score_high = calculate_confidence(example_result_high_confidence)
+     print(f"High confidence example score: {score_high:.2f}")
+ 
+     example_result_medium_confidence = {
+         'metadata': {
+             'source_type': 'blog',
+             'fields_present': 7,
+             'total_expected_fields': 10,
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365), # 1 year old
+             'validated': False
+         }
+     }
+     score_medium = calculate_confidence(example_result_medium_confidence)
+     print(f"Medium confidence example score: {score_medium:.2f}")
+ 
+     example_result_low_confidence = {
+         'metadata': {
+             'source_type': 'unknown',
+             'fields_present': 2,
+             'total_expected_fields': 10,
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365 * 3), # 3 years old
+             'validated': None # Unknown validation
+         }
+     }
+     score_low = calculate_confidence(example_result_low_confidence)
+     print(f"Low confidence example score: {score_low:.2f}")
+ 
+     example_result_missing_data = {
+         'metadata': {
+             'source_type': 'forum',
+             # Missing fields_present, total_expected_fields
+             'discovery_timestamp': None # Missing timestamp
+             # Missing validated
+         }
+     }
+     score_missing = calculate_confidence(example_result_missing_data)
+     print(f"Missing data example score: {score_missing:.2f}")
+ 
+     example_result_future_date = {
+          'metadata': {
+             'source_type': 'official_doc',
+             'fields_present': 10,
+             'total_expected_fields': 10,
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=30), # Future date
+             'validated': True
+         }
+     }
+     score_future = calculate_confidence(example_result_future_date)
+     print(f"Future date example score: {score_future:.2f}")
+ 
+     example_zero_expected_fields = {
+         'metadata': {
+             'source_type': 'official_doc',
+             'fields_present': 5,
+             'total_expected_fields': 0, # Zero expected fields
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=30),
+             'validated': True
+         }
+     }
+     score_zero_expected = calculate_confidence(example_zero_expected_fields)
+     print(f"Zero expected fields example score: {score_zero_expected:.2f}") 
```

---

It seems there was an issue with escaped characters in the docstrings. I'll fix that.

---

```diff
  
  def calculate_completeness_score(fields_present: int, total_expected_fields: int) -> float:
-     \"\"\"Calculates a score based on data completeness.\"\"\"
+     """Calculates a score based on data completeness."""
      if total_expected_fields <= 0:
          return 0.0  # Avoid division by zero, or handle as low confidence
```

```diff
  
  def calculate_reliability_score(source_type: str) -> float:
-     \"\"\"Calculates a score based on source reliability.\"\"\"
+     """Calculates a score based on source reliability."""
      return SOURCE_RELIABILITY_SCORES.get(source_type.lower(), SOURCE_RELIABILITY_SCORES['unknown'])
  
  def calculate_recency_score(discovery_timestamp: datetime.datetime | None) -> float:
-     \"\"\"Calculates a score based on the recency of the information.\"\"\"
+     """Calculates a score based on the recency of the information."""
      if discovery_timestamp is None:
          return 0.5 # Neutral score if timestamp is unknown
  
+     # Ensure discovery_timestamp is offset-aware for comparison with offset-aware datetime.now()
+     if discovery_timestamp.tzinfo is None or discovery_timestamp.tzinfo.utcoffset(discovery_timestamp) is None:
+         # If naive, assume UTC. This might need adjustment based on how timestamps are stored.
+         discovery_timestamp = discovery_timestamp.replace(tzinfo=datetime.timezone.utc)
+ 
      age_in_days = (datetime.datetime.now(datetime.timezone.utc) - discovery_timestamp).days
      if age_in_days < 0: # Timestamp is in the future, treat as very recent or an error
```

```diff
  
  def calculate_validation_score(validated: bool | None) -> float:
-     \"\"\"Calculates a score based on whether the data was validated.\"\"\"
+     """Calculates a score based on whether the data was validated."""
      if validated is None:
          return 0.5 # Neutral score if validation status is unknown
      return 1.0 if validated else 0.0
  
  def calculate_confidence(result_data: dict) -> float:
-     \"\"\"
+     """
      Calculates a confidence score for a given discovery result.
  
```

```diff
      Returns:
          float: A confidence score between 0.0 and 1.0.
-     \"\"\"
+     """
      metadata = result_data.get('metadata', {})
  
```

```diff
  
      # Weighted sum of scores
-     # Ensure weights sum to 1.0 if modifying them. For now, we assume they do.
-     # Normalization: Each individual score is already between 0 and 1.
-     # The weighted sum will also be between 0 and 1 if weights sum to 1.
-     # Current weights sum: 0.3 + 0.4 + 0.15 + 0.15 = 1.0
- 
      confidence_score = (
          completeness * WEIGHTS['completeness'] +
```

```diff
      )
      
-     # Consistency bonus is not yet implemented as it requires cross-result analysis.
-     # For now, the score is capped at 1.0.
      return min(max(confidence_score, 0.0), 1.0)
  
  if __name__ == '__main__':
-     # Example Usage (for direct testing of this module)
+     # Example Usage
      print("Running example confidence score calculations...")
  
      example_result_high_confidence = {
+         'data': {}, # Add dummy data field
          'metadata': {
              'source_type': 'official_doc',
```

```diff
  
      example_result_medium_confidence = {
+         'data': {},
          'metadata': {
              'source_type': 'blog',
              'fields_present': 7,
              'total_expected_fields': 10,
-             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365), # 1 year old
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365),
              'validated': False
          }
```

```diff
  
      example_result_low_confidence = {
+         'data': {},
          'metadata': {
              'source_type': 'unknown',
              'fields_present': 2,
              'total_expected_fields': 10,
-             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365 * 3), # 3 years old
-             'validated': None # Unknown validation
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365 * 3),
+             'validated': None
          }
      }
```

```diff
  
      example_result_missing_data = {
+         'data': {},
          'metadata': {
              'source_type': 'forum',
-             # Missing fields_present, total_expected_fields
-             'discovery_timestamp': None # Missing timestamp
-             # Missing validated
+             'discovery_timestamp': None,
          }
      }
```

```diff
  
      example_result_future_date = {
+         'data': {},
           'metadata': {
              'source_type': 'official_doc',
              'fields_present': 10,
              'total_expected_fields': 10,
-             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=30), # Future date
+             'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(days=30),
              'validated': True
          }
```

```diff
  
      example_zero_expected_fields = {
+         'data': {},
          'metadata': {
              'source_type': 'official_doc',
              'fields_present': 5,
-             'total_expected_fields': 0, # Zero expected fields
+             'total_expected_fields': 0,
              'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=30),
              'validated': True
```

---

The linter errors are fixed.

Next, I'll create the test file `api_discovery_tool/processing/tests/test_scoring.py` with initial test cases.

---

```diff
- 
+ import unittest
+ import datetime
+ from api_discovery_tool.scoring import calculate_confidence, calculate_completeness_score, calculate_reliability_score, calculate_recency_score, calculate_validation_score
+ 
+ class TestScoringSystem(unittest.TestCase):
+ 
+     def test_calculate_completeness_score(self):
+         self.assertEqual(calculate_completeness_score(10, 10), 1.0) # Full completeness
+         self.assertEqual(calculate_completeness_score(5, 10), 0.5)  # Half completeness
+         self.assertEqual(calculate_completeness_score(0, 10), 0.0)  # No completeness
+         self.assertEqual(calculate_completeness_score(10, 0), 0.0) # Edge case: zero total fields
+         self.assertEqual(calculate_completeness_score(0, 0), 0.0)   # Edge case: zero fields
+         self.assertEqual(calculate_completeness_score(12, 10), 1.0) # More fields than expected (capped at 1.0)
+         self.assertEqual(calculate_completeness_score(-5, 10), 0.0) # Negative fields present (capped at 0.0)
+ 
+     def test_calculate_reliability_score(self):
+         self.assertEqual(calculate_reliability_score('official_doc'), 0.9)
+         self.assertEqual(calculate_reliability_score('known_api_db'), 0.8)
+         self.assertEqual(calculate_reliability_score('blog'), 0.5)
+         self.assertEqual(calculate_reliability_score('forum'), 0.3)
+         self.assertEqual(calculate_reliability_score('unknown_source_type'), 0.2) # Default for unknown
+         self.assertEqual(calculate_reliability_score('OFFICIAL_DOC'), 0.9) # Case-insensitivity
+ 
+     def test_calculate_recency_score(self):
+         now = datetime.datetime.now(datetime.timezone.utc)
+         self.assertAlmostEqual(calculate_recency_score(now), 1.0, places=7) # Very recent
+         one_year_ago = now - datetime.timedelta(days=365)
+         self.assertAlmostEqual(calculate_recency_score(one_year_ago), 0.9, places=7) # 1 year old
+         five_years_ago = now - datetime.timedelta(days=5*365)
+         self.assertAlmostEqual(calculate_recency_score(five_years_ago), 0.5, places=7) # 5 years old
+         ten_years_ago = now - datetime.timedelta(days=10*365)
+         self.assertAlmostEqual(calculate_recency_score(ten_years_ago), 0.0, places=7) # 10 years old (max decay)
+         eleven_years_ago = now - datetime.timedelta(days=11*365)
+         self.assertAlmostEqual(calculate_recency_score(eleven_years_ago), 0.0, places=7) # Older than max decay
+         future_date = now + datetime.timedelta(days=30)
+         self.assertAlmostEqual(calculate_recency_score(future_date), 1.0, places=7) # Future date
+         self.assertEqual(calculate_recency_score(None), 0.5) # None timestamp
+         
+         # Test with naive datetime (should be handled by assuming UTC)
+         naive_one_year_ago = datetime.datetime.now() - datetime.timedelta(days=365)
+         self.assertAlmostEqual(calculate_recency_score(naive_one_year_ago), 0.9, places=7)
+ 
+     def test_calculate_validation_score(self):
+         self.assertEqual(calculate_validation_score(True), 1.0)
+         self.assertEqual(calculate_validation_score(False), 0.0)
+         self.assertEqual(calculate_validation_score(None), 0.5) # Neutral for unknown
+ 
+     def test_calculate_confidence_high_confidence(self):
+         data = {
+             'metadata': {
+                 'source_type': 'official_doc',
+                 'fields_present': 10,
+                 'total_expected_fields': 10,
+                 'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=30),
+                 'validated': True
+             }
+         }
+         # Expected: (1.0 * 0.3) + (0.9 * 0.4) + (approx 0.99 * 0.15) + (1.0 * 0.15)
+         # = 0.3 + 0.36 + 0.1485 + 0.15 = 0.9585
+         self.assertAlmostEqual(calculate_confidence(data), 0.958, delta=0.001)
+ 
+     def test_calculate_confidence_low_confidence(self):
+         data = {
+             'metadata': {
+                 'source_type': 'unknown',
+                 'fields_present': 2,
+                 'total_expected_fields': 10,
+                 'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=3*365),
+                 'validated': False
+             }
+         }
+         # Expected: (0.2 * 0.3) + (0.2 * 0.4) + (0.7 * 0.15) + (0.0 * 0.15)
+         # = 0.06 + 0.08 + 0.105 + 0.0 = 0.245
+         self.assertAlmostEqual(calculate_confidence(data), 0.245, delta=0.001)
+ 
+     def test_calculate_confidence_medium_confidence_blog(self):
+         data = {
+             'metadata': {
+                 'source_type': 'blog',
+                 'fields_present': 7,
+                 'total_expected_fields': 10,
+                 'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=365),
+                 'validated': False
+             }
+         }
+         # Expected: (0.7 * 0.3) + (0.5 * 0.4) + (0.9 * 0.15) + (0.0 * 0.15)
+         # = 0.21 + 0.20 + 0.135 + 0.0 = 0.545
+         self.assertAlmostEqual(calculate_confidence(data), 0.545, delta=0.001)
+ 
+     def test_calculate_confidence_missing_metadata(self):
+         data = {
+             'metadata': {}
+         }
+         # Expected (defaults): (0/0->0.0 * 0.3) + (unknown->0.2 * 0.4) + (None->0.5 * 0.15) + (None->0.5 * 0.15)
+         # = 0.0 + 0.08 + 0.075 + 0.075 = 0.23
+         self.assertAlmostEqual(calculate_confidence(data), 0.23, delta=0.001)
+     
+     def test_calculate_confidence_no_metadata_key(self):
+         data = {}
+         # Should behave same as empty metadata dict
+         self.assertAlmostEqual(calculate_confidence(data), 0.23, delta=0.001)
+ 
+     def test_calculate_confidence_zero_expected_fields(self):
+         data = {
+             'metadata': {
+                 'source_type': 'official_doc',
+                 'fields_present': 5,
+                 'total_expected_fields': 0, # Completeness score will be 0
+                 'discovery_timestamp': datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=30),
+                 'validated': True
+             }
+         }
+         # Expected: (0.0 * 0.3) + (0.9 * 0.4) + (approx 0.99 * 0.15) + (1.0 * 0.15)
+         # = 0.0 + 0.36 + 0.1485 + 0.15 = 0.6585
+         self.assertAlmostEqual(calculate_confidence(data), 0.658, delta=0.001)
+ 
+ if __name__ == '__main__':
+     unittest.main() 
```

---

The initial implementation and tests for the scoring module are in place.

I will now log the progress.

---



---

The initial implementation of the confidence scoring system and its tests are now in place. The next step would be to run the tests. If the tests pass, we can consider this subtask complete unless further refinements (like the consistency bonus) are required.

Would you like me to proceed with running the tests?

---

_**User**_

Yes, please proceed with the testing

---

_**Assistant**_



---

Okay, I will now run the tests for the scoring module.

---

```bash
python -m unittest api_discovery_tool/processing/tests/test_scoring.py
```

```
╭─     ~/dev/projects/api-discovery-tool     main ?3 ································· ✔  12:53:15   
╰─ python -
```

---

The test results are not showing in the output. It seems like the command might have been truncated or there was an issue with capturing the output.

I'll try running the tests again, specifically requesting verbose output, which might help ensure the results are captured.

---

```bash
python -m unittest -v api_discovery_tool/processing/tests/test_scoring.py
```

```


╭─     ~/dev/projects/api-discovery-tool     main ?5 ······························· 1 ✘  12:53:17   
❯ python -m unittest -v api_discovery_tool/processing/tests/test_scoring.py
test_calculate_completeness_score (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test_calcula
te_completeness_score) ... ok
test_calculate_confidence_high_confidence (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test
_calculate_confidence_high_confidence) ... ok
test_calculate_confidence_low_confidence (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test_
calculate_confidence_low_confidence) ... ok
test_calculate_confidence_medium_confidence_blog (api_discovery_tool.processing.tests.test_scoring.TestScoringSyst
em.test_calculate_confidence_medium_confidence_blog) ... ok
test_calculate_confidence_missing_metadata (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.tes
t_calculate_confidence_missing_metadata) ... ok
test_calculate_confidence_no_metadata_key (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test
_calculate_confidence_no_metadata_key) ... ok
test_calculate_confidence_zero_expected_fields (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem
.test_calculate_confidence_zero_expected_fields) ... ok
test_calculate_recency_score (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test_calculate_re
cency_score) ... FAIL
test_calculate_reliability_score (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test_calculat
e_reliability_score) ... ok
test_calculate_validation_score (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test_calculate
_validation_score) ... ok

======================================================================
FAIL: test_calculate_recency_score (api_discovery_tool.processing.tests.test_scoring.TestScoringSystem.test_calcul
ate_recency_score)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/cryophobic/dev/projects/api-discovery-tool/api_discovery_tool/processing/tests/test_scoring.py", li
ne 28, in test_calculate_recency_score
    self.assertAlmostEqual(calculate_recency_score(one_year_ago), 0.9, places=7) # 1 year old
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: 0.9000684462696783 != 0.9 within 7 places (6.844626967827949e-05 difference)

----------------------------------------------------------------------
Ran 10 tests in 0.001s

FAILED (failures=1)

╭─     ~/dev/projects/api-discovery-tool     main ?5 ······························· 1 ✘  12:53:20   
╰─
```

---



---

The tests revealed a failure in `test_calculate_recency_score` due to a small precision difference. The scoring logic uses `365.25` days per year for calculating age, while the test used `365` days and expected an exact multiple of the decay rate.

I'll

---

